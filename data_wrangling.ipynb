{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import fastparquet\n",
    "import pyarrow\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import string\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing training and test data\n",
    "train_df=pd.read_parquet('./data/train-00000-of-00001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TeliaSonera TLSN said the offer is in line wit...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMEN...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clothing retail chain Sepp+ñl+ñ 's sales incre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lifetree was founded in 2000 , and its revenue...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nordea Group 's operating profit increased in ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Seven-month sales of Ragutis , which is contro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>The OMX Helsinki index was 0.33 pct lower at 9...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>In the Baltic states the company reports net s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>The company said that its comparable operating...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0    TeliaSonera TLSN said the offer is in line wit...      2\n",
       "1    STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMEN...      2\n",
       "2    Clothing retail chain Sepp+ñl+ñ 's sales incre...      2\n",
       "3    Lifetree was founded in 2000 , and its revenue...      2\n",
       "4    Nordea Group 's operating profit increased in ...      2\n",
       "..                                                 ...    ...\n",
       "964  Seven-month sales of Ragutis , which is contro...      0\n",
       "965  The OMX Helsinki index was 0.33 pct lower at 9...      0\n",
       "966  In the Baltic states the company reports net s...      0\n",
       "967  The company said that its comparable operating...      0\n",
       "968  LONDON MarketWatch -- Share prices ended lower...      0\n",
       "\n",
       "[969 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_parquet('./data/test-00000-of-00001.parquet')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agreement was signed with Biohit Healthcare Ltd , the UK-based subsidiary of Biohit Oyj , a Finnish public company which develops , manufactures and markets liquid handling products and diagnostic test systems .\n"
     ]
    }
   ],
   "source": [
    "print(train_df['sentence'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'agreement',\n",
       " 'was',\n",
       " 'signed',\n",
       " 'with',\n",
       " 'Biohit',\n",
       " 'Healthcare',\n",
       " 'Ltd',\n",
       " ',',\n",
       " 'the',\n",
       " 'UK-based',\n",
       " 'subsidiary',\n",
       " 'of',\n",
       " 'Biohit',\n",
       " 'Oyj',\n",
       " ',',\n",
       " 'a',\n",
       " 'Finnish',\n",
       " 'public',\n",
       " 'company',\n",
       " 'which',\n",
       " 'develops',\n",
       " ',',\n",
       " 'manufactures',\n",
       " 'and',\n",
       " 'markets',\n",
       " 'liquid',\n",
       " 'handling',\n",
       " 'products',\n",
       " 'and',\n",
       " 'diagnostic',\n",
       " 'test',\n",
       " 'systems',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting an example of initial tokenization\n",
    "word_tokenize(train_df['sentence'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89642,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the length of corpus and vocabulary from initial tokenization\n",
    "corpus = [word_tokenize(doc) for doc in train_df['sentence']]\n",
    "import itertools\n",
    "flattenedcorpus_tokens = pd.Series(list(itertools.chain(*corpus)))\n",
    "flattenedcorpus_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11435"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattenedcorpus_tokens.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".          3879\n",
       ",          3762\n",
       "the        3735\n",
       "of         2551\n",
       "in         2180\n",
       "and        2079\n",
       "to         1998\n",
       "a          1315\n",
       "The        1093\n",
       "for         891\n",
       "'s          794\n",
       "is          740\n",
       "EUR         716\n",
       "will        689\n",
       "company     642\n",
       "from        599\n",
       "on          531\n",
       "its         481\n",
       "has         459\n",
       "with        454\n",
       "said        447\n",
       "by          438\n",
       "be          434\n",
       ")           411\n",
       "Finnish     410\n",
       "(           409\n",
       "as          407\n",
       "mn          399\n",
       "``          377\n",
       "%           353\n",
       "at          351\n",
       "that        345\n",
       "million     343\n",
       "sales       336\n",
       "profit      306\n",
       ":           304\n",
       "was         296\n",
       "it          295\n",
       "net         280\n",
       "Finland     272\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting high frequency words from initial tokenization\n",
    "flattenedcorpus_tokens.value_counts()[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "under     39\n",
       "above     12\n",
       "below     14\n",
       "up       141\n",
       "down      73\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial tokenation results have high stopwords frequencies \n",
    "#we'd like to remove stopwords in preproccessing,however certain stopwords may be considerred key in differenciating auditors' sentiment, \n",
    "#examples below show up in a rather decent frequency in our corpus  \n",
    "audit_nonstop=['under','above','below','up','down']\n",
    "flattenedcorpus_tokens.value_counts()[audit_nonstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s    794\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 's also has too high of a frequency and lack of useful meaning, to remove\n",
    "flattenedcorpus_tokens.value_counts()[[\"'s\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'to',\n",
       " 'from',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " \"'s\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom stopwords to remove\n",
    "audit_stopwords=stopwords.words('english')\n",
    "for word in audit_nonstop:\n",
    "        audit_stopwords.remove(word)\n",
    "for punct in string.punctuation:\n",
    "        audit_stopwords.append(punct)\n",
    "audit_stopwords.append(\"'s\")\n",
    "audit_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altia a False\n",
      "'s ' False\n",
      "operating o False\n",
      "profit p False\n",
      "jumped j False\n",
      "to t False\n",
      "EUR e False\n",
      "47 4 True\n",
      "million m False\n",
      "from f False\n",
      "EUR e False\n",
      "6.6 6 True\n",
      "million m False\n",
      ". . False\n"
     ]
    }
   ],
   "source": [
    "# aside from stopwords, numbers typically have a lack of useful meaning too\n",
    "# inspecting tokenized example with numbers to build function in the custom preprocessing transformer later on to remove them\n",
    "digits=['0','1','2','3','4','5','6','7','8','9']\n",
    "for token in word_tokenize(train_df['sentence'][0]):\n",
    "    print(token,token.lower()[0],token.lower()[0] in (digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building function to tokenize, remove stopwords, and remove tokens starting with numbers \n",
    "def pre_process(doc):\n",
    "    doc_norm = [token.lower() for token in word_tokenize(doc) if (token.lower() not in audit_stopwords) and (token.lower()[0] not in digits)]\n",
    "    return doc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1=train_df['sentence'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['altia', 'operating', 'profit', 'jumped', 'eur', 'million', 'eur', 'million']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47478,)\n"
     ]
    }
   ],
   "source": [
    "# comparing corpus and vocabulary sizes before/after removing stopwords and numbers, which are significantly reduced\n",
    "flattenedcorpus_1 = pd.Series(list(itertools.chain(*corpus1)))\n",
    "print(flattenedcorpus_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89642,)\n"
     ]
    }
   ],
   "source": [
    "print(flattenedcorpus_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8699"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattenedcorpus_1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattenedcorpus_tokens.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step of preprocessing is lemmatization, we will use wordnet lemmatization, \n",
    "# for this, we'd need the wordnet part of speach tags converted from nltk tags\n",
    "# function to tag each nltk part of speech tag to wordnet\n",
    "def wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to lemmatize using wordnet\n",
    "def lemmatize(doc_norm):\n",
    "    wnl=WordNetLemmatizer()\n",
    "    wn_tagged=list(map(lambda x: (x[0],wordnet_pos(x[1])),pos_tag(doc_norm)))\n",
    "    lemmatized_norm=[wnl.lemmatize(token, pos) for (token, pos) in wn_tagged if pos is not None]\n",
    "    return \" \".join(lemmatized_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  altia operating profit jump eur eur\n",
       "1    agreement sign biohit healthcare ltd uk-based ...\n",
       "2    kesko pursues strategy healthy focus growth co...\n",
       "3    vaisala headquarter helsinki finland develop m...\n",
       "4       also six-year historic analysis provide market\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting examples after lemmatization\n",
    "corpus2=train_df['sentence'].apply(pre_process).apply(lemmatize)\n",
    "corpus2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building custom preprocessing transformer to lower case, remove custom stopwords, and lemmatize\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def fit(self, data, y = 0):\n",
    "        return self\n",
    "    \n",
    "    def process_doc(self, doc):\n",
    "        doc_norm=pre_process(doc)\n",
    "        lemmatized_norm=lemmatize(doc_norm)\n",
    "        return lemmatized_norm\n",
    "    def transform(self, data, y = 0):\n",
    "        fully_normalized_corpus = data.apply(self.process_doc)\n",
    "       \n",
    "        return fully_normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc=TextPreprocessor()\n",
    "pre_processed=pre_proc.fit_transform(train_df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc_split=[sent.split() for sent in pre_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final corpus contains 45051 words, with 7265 unique values in the dictionary\n"
     ]
    }
   ],
   "source": [
    "# taking a look at the final corpus and vocab size\n",
    "flattenedcorpus_3=pd.Series(itertools.chain(*pre_proc_split))\n",
    "print(f\"Final corpus contains {len(flattenedcorpus_3)} words, with {len(flattenedcorpus_3.unique())} unique values in the dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
